- The Root Cause:
  Understanding the root cause is essential for performing the long-term 
  remediation.

It might seem that once you have a reproduction case, you already know the root 
cause of the problem.
But more often than not, it's not true.

In our overloaded server example, we figured out that the backup system was 
blocking the websites from working, and so we mitigated that immediate problem 
to unblock the user. 

But we didn't really look into the root cause of our server being stuck.

This could be because: 
· The network bandwidth is saturated, 
· The disk transfer is too slow, 
· The hard drive is faulty, 
· or a bunch of other reasons.

We also didn't do anything to make sure our backups could run successfully in 
the future. 


* So how do we go about finding the actual root cause of the problem?

We generally follow a cycle:
· We look at the information we have.
· We come up with a hypothesis that could explain the problem
· We then test our hypothesis.

If we confirm our theory, we found the root cause. 
If we don't, then we go back to the beginning and try different possibilities.

To get inspired and come up with ideas of possible causes: 
· We look at information that we have already, and gather more if needed.
· We could search online for the error messages that we get.
· We look at the documentation of the applications involved.

Whenever possible, we should test our hypothesis on a test environment instead 
of the production environment that our users are working with.
That way, we avoid interfering with what our users are doing and we can tinker 
around without fear of breaking something important.

Depending on what you're trying to fix, this might mean we have to try our code 
in a newly installed machine, spinning up a test server, using test data, and 
so on.
It can take some time to get the setup, but the extra safety is definitely 
worth it. 

Even when it seems that the error might be related to the specific production 
environment, it's always a good idea to check if we can reproduce the problem 
in a test environment before we modify production.

In our overloaded server example, if the problem is with the hardware, we 
wouldn't be able to replicate it with a test server.

In that case, we would need to either wait until the services aren't being used 
or bring up a secondary server, migrate the services there, and only then look 
at what's wrong with the computer.

On the flip side, if the problem is related to some configuration of either the 
web services or the backup service, we'd still see it in the test server.

So, we would always start by, setting up a test instance of the service and 
checking if the problem replicates there before touching the production 
instance.

So say we have a test server running the same websites.
When we start the backup, we see that the website stop responding.
This is great because we have a reproduction case and we can debug it properly. 


* How do we find the root cause? 

One possible culprit could be too much disk input and output. 

To get more info on this, we could use: 
· iotop, which is a tool similar to top that lets us see which processes are 
  using the most input and output. 

Other related tools are: 
· iostat
· vmstat 

These tools show statistics on the input/output operations and the virtual 
memory operations. 

If the issue is that the process generates too much input or output, we could 
use a command like:
· ionice to make our backup system reduce its priority to access the disk and 
  let the web services use it too.


* What if the input and output is not the issue? 

Another option would be that the service is using too much network because it's 
transmitting the data to be backed up to a central server and that transmission 
blocks everything else.

We can check this using:
· iftop, yet another tool similar to top that shows the current traffic on the 
  network interfaces. 

If the backup is eating all the network bandwidth, we could look at the 
documentation for the backup software and check if it already includes an 
option to limit the bandwidth. 

The "rsync" command, which is often used for backing up data, includes a 
"-bwlimit", just for this purpose.

If that option isn't available, we can use a program like "Trickle" to limit 
the bandwidth being used.


* But what if the network isn't the issue either? 

We need to put our debugging creativity to work, and come up with other 
possible reasons for why it's failing. 

It could be that the compression algorithms selected is too aggressive and 
compressing the backups is using all of the server's processing power.

We could solve this by reducing the compression level or using the "nice" 
command to reduce the priority of the process and accessing the CPU.
If that's still not the case, we need to keep looking, check the logs to see if 
we find anything that we missed before. 

Maybe look online for other people dealing with similar problems related to 
interactions of the backing up software with the web surfing software, and keep 
doing this until we come up with something that could be causing our problem.


I know this sounds like a lot of work, but it's usually not that bad. 
In general, by using the tools available to us, we can find enough info to land 
on the right hypothesis after only a few tries and with experience, we'll get 
better at picking up the most likely hypothesis the first time around. 