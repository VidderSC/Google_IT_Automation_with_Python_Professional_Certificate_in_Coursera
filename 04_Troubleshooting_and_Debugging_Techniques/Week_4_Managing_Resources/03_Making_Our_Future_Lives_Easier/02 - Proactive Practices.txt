- Proactive Practices:

Something that IT specialists and exterminators have in common is dealing with bugs. 

It can be bugs in our software or someone else's software. 
But we'll come across lots of bugs that trigger lots of different failures in our programs. 

There's a bunch of strategies we can adopt to make our lives easier, by catching issues before they affect our users or making troubleshooting simpler by having better information. 
We've touched upon some of them here and there but now, it's time to deep dive. 

To avoid having to scramble to fix things when there's an outage, it's really helpful to have infrastructure that lets us test changes in advance so that we can check that things are working as expected before they reach our users. 

If we're the ones writing the code, one thing we can do is to make sure that our code has good "unit tests" and "integration tests". 
If our tests have good coverage of the code, we can rely on them to catch a wide array of bugs whenever there's a change that may break things. 

For these tests to be really meaningful, we need to run them often, and make sure we know as soon as they fail. 
Setting up "continuous integration" (CI) can help with that. 

Another step in this direction is to have a "test environment", where we can deploy new code before shipping it to the rest of our users. This serves two purposes. 
- First, we can do a thorough check of the software as it will be seen by the 
  users. Depending on the software and how often we update it, we can do both automated and manual tests in this environment. 
- Second, we can use this test environment to troubleshoot problems whenever 
  they happen. We can try possible solutions and new features without affecting the production environment. 
  
Taking this even further, another recommended practice when managing a fleet of computers is to deploy software in "phases" or "canaries". 

What this means is, that instead of upgrading all computers at the same time and possibly breaking all of them at the same time, you upgrade some computers first and check how they behave. 
If everything goes fine, you can upgrade a few more, and so on until you're confident enough to upgrade the remaining part of the fleet. 

As the saying goes, like a canary in a coal mine. To make the best use of this practice, we'll need to be able to easily roll back to the previous version. 

Depending on the software, this might require more or less infrastructure. But trust me, it's worth spending the time setting up that additional infrastructure. 

If you deploy a software version that was broken and suddenly a bunch of your computers aren't working correctly, you'll want to roll them back to a previous state as fast as possible. 

Now, even with all these preventative measures, bugs will still filter through and problems will occur. We can make our troubleshooting easier by including good debug logging in the code. That way, whenever we have to figure out an issue, we can look at the logs and get a pretty good idea of what's going on.

Another method that can help us is, having centralized logs collection. 

This means there's a special server that gathers all the logs from all the servers or even all the computers in the network. That way, when we have to look at those logs, we don't need to connect to each machine individually, we can comb through all the logs together in a centralized server. 

Similarly, having a good monitoring system can be super helpful. We can use it to catch issues early before they affect too many users. 

During a debugging session, we can look at the collected data to try to determine if there's anything out of the ordinary going on. 

We called out ticketing systems a few times already, because we can't stress their importance enough. Making good use of them can help us save a lot of time when trying to get to the bottom of a problem. 

If we ask users to provide the needed information up front, we don't have to waste time and go back and forth. Even here, we can look at opportunities for automation. 

Say you almost always want some specific info from the users computers, you can automate getting it by creating a script that gathers all the data you want and have the users attach it to the ticket. 

Finally, remember to spend time writing documentation. Just as importantly, store the documentation in a well-known location. Even if writing documentation isn't especially fun, having good instructions on how to solve a specific problem, knowing how to diagnose what's going on with the server, or tracking the known issues in a system can be real time savers. 

At Google, we have a bunch of docs called "Playbooks" where we detail what a person who's on call can do to diagnose and mitigate a ton of different problems. 
By keeping this information updated, we make sure that no matter who the person on call is, everybody has access to the knowledge base accumulated by the whole team. 

It doesn't stop there. If we're dealing with systems that change and grow, we can proactively plan for the additional capacity that we'll need in the future.