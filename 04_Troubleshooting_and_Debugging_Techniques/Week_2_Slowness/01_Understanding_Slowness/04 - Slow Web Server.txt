A user has alerted us that one of the web servers in our company is being slow, 
and we need to figure out what's going on. 
Let's start by navigating to the website and loading the page.

site.example.com


We see that the page loads. It seems to be a little slow but it's hard to 
measure this on our own.
Let's use a tool called "ab" which stands for "Apache Benchmark" tool to figure 
out how slow it is. 
We'll run "ab -n 500" to get the average timing of 500 requests, and then pass 
our "site.example.com" for the measurement. 
This tool is super useful for checking if a website is behaving as expected or 
not. It will make a bunch of requests and summarize the results once it's done. 
Here, were asking for it to do 500 requests to our website. 

There are a lot more options that we could pass like how many requests we want 
the program to do at the same time, or if the test to finish after timeout, 
even if not all requests completed, we're making 200 requests so that we can 
get an average of how long things are taking. 
Once the test finishes, we can look at the data and decide if it's actually 
slow or not. 

"""
$ ab -n 200 https://www.login.com/
This is ApacheBench, Version 2.3 <$Revision: 1879490 $>
Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/
Licensed to The Apache Software Foundation, http://www.apache.org/

Benchmarking www.login.com (be patient)
Completed 100 requests
Completed 200 requests
Finished 200 requests


Server Software:        nginx
Server Hostname:        www.login.com
Server Port:            443
SSL/TLS Protocol:       TLSv1.2,ECDHE-RSA-AES256-GCM-SHA384,2048,256
Server Temp Key:        X25519 253 bits
TLS Server Name:        www.login.com

Document Path:          /
Document Length:        202464 bytes

Concurrency Level:      1
Time taken for tests:   215.349 seconds
Complete requests:      200
Failed requests:        0
Total transferred:      40602712 bytes
HTML transferred:       40492800 bytes
Requests per second:    0.93 [#/sec] (mean)
Time per request:       155.745 [ms] (mean)
Time per request:       155.745 [ms] (mean, across all concurrent requests)
Transfer rate:          184.12 [Kbytes/sec] received

Connection Times (ms)
              min  mean[+/-sd] median   max
Connect:      439  503  78.0    512    1487
Processing:   501  574  66.0    584    1229
Waiting:      138  157   9.6    160     206
Total:        941 1077 118.7   1096    2035

Percentage of the requests served within a certain time (ms)
  50%   1096
  66%   1103
  75%   1106
  80%   1107
  90%   1113
  95%   1123
  98%   1430
  99%   1996
 100%   2035 (longest request)
"""

All right. The tool has finished running the 200 requests. We see that the mean 
time per requests was a 155 milliseconds. While this is not a super huge 
number, it's definitely more than what we'd expect for such a simple website. 
It seems that something is going on with the web server and we need to 
investigate further. 

Let's connect to the web server and check out what's going on.

$ ssh webserver


We'll start by looking at the output of "top" and see if there's anything 
suspicious there.

We see that there's a bunch of "ffmpeg" processes running, which are basically 
using all the available CPU. 
See those load numbers? Thirty is definitely not normal. 
Remember that the load average on Linux shows how much time the processor is 
busy at a given minute with one meaning it was busy for the whole minute. 
This computer has two processors. So any number above two means that it's 
overloaded. 
During each minute, there were more processes waiting for processor time than 
the processor had to give. 

This ffmpeg program is used for video transcoding which means converting files 
from one video format to another. This is a CPU intensive process and seems 
like the likely culprit for our server being overloaded. 

- What can we do? 
One thing we can try is to change the processes priorities so that the web 
server takes precedence. 
The process priorities in Linux are so that the lower the number, the higher 
the priority. Typical numbers go from 0 to 19. 
By default, processes start with a priority of zero. But we can change that 
using the "nice" and "renice" commands. 
We use "nice" for starting a process with a different priority and "renice" for 
changing the priority of a process that's already running. 

Let's exit top with queue and change the priorities. 
We want to run "renice" for all the "ffmpeg" processes that are running right 
now. 
We could do this one by one. But it would be manual, error-prone, and super 
boring. 
Instead, we can use a quick line of shell script to do this for us. 
For that, we'll use the "pidof" command that receives the process name and 
returns all the process IDs that have that name. We'll iterate over the output 
of the pidof command with a for loop and then call renice for each of the 
process IDs. 
Renice takes the new priority as the first argument, and the process ID to 
change as the second one. In our case, we'll want the lowest possible priority 
which is 19. 

"""
$ for pid in $(pidof ffmpeg); do renice 19 $pid; done
2874 (process ID) old priority 0, new priority 19
2869 (process ID) old priority 0, new priority 19
2864 (process ID) old priority 0, new priority 19
2859 (process ID) old priority 0, new priority 19
2854 (process ID) old priority 0, new priority 19
"""

We see that the priorities for those processes were updated. 

Let's run our benchmarking software again and check out if it made any 
difference.

This time, the meantime is 153 milliseconds. It doesn't seem like our renice 
helped. Apparently, the OS is still giving these ffmpeg processes way too much 
processor time. 
Our website is still slow. 

- What else can we do? 
These transcoding processes are CPU intensive, and running them in parallel is 
overloading the computer. 
So one thing we could do is, modify whatever's triggering them to run them one 
after the other instead of all at the same time. 

To do that, we'll need to find out how these processes got started. 

Â· First, we'll look at the output of the "ps" command to get some more 
  information about the processes. 
  We'll call "ps ax" which shows us all the running processes on the computer, 
  and we'll connect the output of the command to less, to be able to scroll 
  through it.

$ ps ax | less

Now we'll look for the ffmpeg process using slash "/" which is the search key 
when using less.

"""
2829 ?   SNsl  4:43 /usr/bin/ffmpeg -nostats -nostdin -i static/001.webm static/001.mp4
2834 ?   SNsl  4:55 /usr/bin/ffmpeg -nostats -nostdin -i static/002.webm static/002.mp4
2839 ?   SNsl  4:46 /usr/bin/ffmpeg -nostats -nostdin -i static/003.webm static/003.mp4
"""

We see that there are a bunch of ffmpeg processes that are converting videos 
from the "webm" format to the "mp4" format. 
We don't know where these videos are on the hard drive. We can try using the 
locate command to see if we can find them. 

We'll first exit the less interface with "q" and then call:

$ locate static/001.webm

We see that the static directory is located in the server deploy videos 
directory. Let's change into that directory and see what we find.

There's a bunch of files here. We could check them all one-by-one to see if one 
of them contained a call to ffmpeg. But that sounds like a lot of manual work.
Instead, let's use "grep" to check if any of these files contains a call to 
"ffmpeg".

$ grep ffmpeg *
deploy.sh:# This script runs ffmpeg in parallel to convert all of the webm files to mp4.
deploy.sh:    daemonize -c $PWD /usr/bin/ffmpeg -nostats -nostdin -i "$video" "$mp4_video"

So we see that there's a couple of mentions in the "deploy.sh" file. 

Let's take a look at that one. Since we're connecting to the server remotely, 
we can't open the file using a graphical editor. We need to use a command line 
editor instead. We'll use vim in this case.

$ vim deploy.sh

We see that this script is starting the ffmpeg processes in parallel using a 
tool called "daemonize" that runs each program separately as if it were a 
daemon. 
This might be okay if we only need to convert a couple of videos but launching 
one separate process for each of the videos in the static directory is 
overloading our server. 
So we want to change this to run only one video conversion process at a time. 
We'll do that by simply deleting the daemonized part and keeping the part that 
calls ffmpeg, then save and exit.

"""
/usr/bin/ffmpeg -nostats -nostdin -i "$video" "$mp4_video"

"""

We've modified the file. But this won't change the processes that are already 
running. We want to stop these processes but not cancel them completely, as 
doing so would mean that the videos being converted right now will be 
incomplete. 
So we'll use the "killall" command with the "-STOP" flag which sends a stop 
signal but doesn't kill the processes completely.

$ killall -STOP ffmpeg

We now want to run these processes one at a time. 

- How can we do that? 
We could send the "CONT" signal to one of them, wait till it's done, and then 
send it to the next one. But that's a lot of manual work. 

- Can be automate it? 
Yes. But it's a little tricky. So pay close attention. We can iterate through 
the list of processes using the same for loop with the pit of command that we 
used earlier.

Inside the for loop, we want to send the cont signal and then wait until the 
process is done. Unfortunately, there's no command to wait until the process 
finishes. 
But we can create a while loop that sends the cont signal to the process. This 
will succeed as long as the process exists, and fails once the process goes 
away. 
Inside this while loop, we'll simply add a call to sleep one, to wait one 
second until the next check.

$ for pid in $(pidof ffmpeg); do while kill -CONT $pid; do sleep 1; done ; done


Okay. Now our server is running one ffmpeg process at a time. Let's turn our 
benchmark once more.

The mean time is now 33 milliseconds. That's much lower than before. 
We've managed to get our web server to reply promptly to the request again. 

We've mentioned a few different approaches that we can take when we can't fix 
the code like renicing the processes, or running them one after the other when 
that doesn't help.